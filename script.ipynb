{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='bpl.circdata-solutions.co.uk', port=443): Max retries exceeded with url: /Fusion/login.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001A6AA9B5760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dns_host, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 954\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001A6AA9B5760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    441\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    442\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    443\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    444\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    445\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    449\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    450\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    453\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:785\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    783\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 785\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    786\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    787\u001b[0m )\n\u001b[0;32m    788\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='bpl.circdata-solutions.co.uk', port=443): Max retries exceeded with url: /Fusion/login.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001A6AA9B5760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Backend\\Python\\web_scrape\\Product\\script.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 365>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=362'>363</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=363'>364</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTo run analysis please provide colours for the company: \u001b[39m\u001b[39m{\u001b[39;00mclient_info[\u001b[39m\"\u001b[39m\u001b[39mcompany\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=364'>365</a>\u001b[0m get_basic_info(company, stats_url\u001b[39m=\u001b[39;49mstats_url)\n",
      "\u001b[1;32mc:\\Backend\\Python\\web_scrape\\Product\\script.ipynb Cell 1\u001b[0m in \u001b[0;36mget_basic_info\u001b[1;34m(company, stats_url)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_basic_info\u001b[39m(company, stats_url):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m     \u001b[39m## Login to dynamail\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m     webpage_response \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39;49mget(credentials[company][\u001b[39m'\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m     webpage \u001b[39m=\u001b[39m webpage_response\u001b[39m.\u001b[39mcontent\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m     \u001b[39m# login_page = BeautifulSoup(webpage,'html.parser')\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Backend/Python/web_scrape/Product/script.ipynb#W0sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m         \u001b[39m##login input fields\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\requests\\api.py:75\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39m\u001b[39mget\u001b[39m\u001b[39m'\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    524\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    525\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[0;32m    526\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[0;32m    527\u001b[0m }\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    648\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\JudeUbah\\anaconda3\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='bpl.circdata-solutions.co.uk', port=443): Max retries exceeded with url: /Fusion/login.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001A6AA9B5760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "from math import inf\n",
    "import os\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "import csv\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "credentials ={\n",
    "    'ite': {\n",
    "        'password':'ITCrowdRules!',\n",
    "         'url': 'https://iteuropa.circdata-solutions.co.uk/Fusion/login.aspx',\n",
    "         '__VIEWSTATE' : '/wEPDwUKMTgwMjQwMDI5M2Rksovo2zHemSveuJw8vAuQ6Xf8InA=',\n",
    "         '__EVENTVALIDATION' : '/wEWBQKPgenHBQLj+sxGAuOoubQBAqqg8cUOAteSgYoCZ204OWUV6uw8bUTN6GYNlbZWM7E=',\n",
    "        'bot_links':['https://www.iteuropa.com/privacy-policy?ref=email_footer','mailto:clients@ite-mail1.com', 'http://www.iteuropa.com', 'https://www.iteuropa.com?ref=email_footer','http://www.iteuropa.com/?q=privacy-policy','http://www.iteuropa.com/','http://www.iteuropa.com/communication-preferences?email=[CD|emailaddress|]']\n",
    "        },\n",
    "    'bpl':  {\n",
    "        'password':'Acumagic10', \n",
    "        'url': 'https://bpl.circdata-solutions.co.uk/Fusion/login.aspx',\n",
    "        '__VIEWSTATE':'/wEPDwUKMTgwMjQwMDI5M2Rk2e1SYhd3ZcfIKWr+jOok9jgKqtU=',\n",
    "        '__EVENTVALIDATION':'/wEWBQLV2djbAQLj+sxGAuOoubQBAqqg8cUOAteSgYoC7owPjxk9TLLjh/iR0zoJPMA+4Yk=',\n",
    "        'bot_links':['http://www.bpl-business.com','http://www.bpl-business.com/privacy-policy.shtml','mailto:weeklynews@bpl-mail1.co.uk']\n",
    "        \n",
    "}\n",
    "}\n",
    "\n",
    "username = 'jubah@bpl-business.com'\n",
    "company = 'bpl'\n",
    "stats_url = 'https://bpl.circdata-solutions.co.uk/Fusion/Dynamail/MailshotDetails.aspx?id=918871c8-d3d8-4ae2-a181-33f1ad223d14'\n",
    "\n",
    "clients = {\n",
    "    'Union Street': ['#00509f', '#e6aaaa'],\n",
    "    'Akixi': ['#035496', '#efc52f']\n",
    "}\n",
    "\n",
    "def isBannedCharacter(character):\n",
    "    if ord(character) >= 128:\n",
    "        return True\n",
    "\n",
    "def remove_characters(string):\n",
    "    unallowed = ['#', '<', '>', '%', '&', '{', '}', '/', '$', '!', '\\'', '\\\"', ':', '@','*','?','\\\\','+','+','`','|','=','â€“' ]\n",
    "    sanitised_string =''\n",
    "    for character in range(len(string)):\n",
    "        if not string[character] in unallowed and not isBannedCharacter(string[character]):\n",
    "            sanitised_string = sanitised_string + string[character]\n",
    "    return sanitised_string\n",
    "\n",
    "\n",
    "def analysis_and_visualisation(company, subject_line, colours, generic_df):\n",
    "    \n",
    "    year = date.today().year\n",
    "    month = date.today().month\n",
    "    day = date.today().day\n",
    "    time_stamp = f\"{day}-{month}-{year}\"\n",
    "\n",
    "    path = os.getcwd()\n",
    "    pixels_path = path + \"\\\\pixels\"\n",
    "    company_path = path + \"\\\\company\"\n",
    "\n",
    "    pixel_files = glob.glob(os.path.join(pixels_path, \"*.csv\"))\n",
    "    comp_files = glob.glob(os.path.join(company_path, \"*.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "    df_px = pd.concat((pd.read_csv(file, parse_dates=[1,2,4],index_col=False) for file in pixel_files), ignore_index=True)\n",
    "    df_comp = pd.concat((pd.read_csv(file, parse_dates=[1,2,4],index_col=False) for file in comp_files), ignore_index=True)\n",
    "    #grab all files\n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "    #########################################################################################################\n",
    "    df = df_comp[~df_comp.Email.isin(df_px.Email)].reset_index(drop=True)\n",
    "\n",
    "    if df.empty:\n",
    "        end=''\n",
    "        print('dataframe is empty - no clicks')\n",
    "    else:\n",
    "    \n",
    "        #Remove pixel emails from the main dataframe\n",
    "        #########################################################################################################\n",
    "\n",
    "\n",
    "        listed_columns = list(df.columns)\n",
    "        df.drop(df.columns[listed_columns.index('BounceDate'):], axis=1, inplace=True)\n",
    "        #(\n",
    "        # The lambda function can be used to modify the whole df.\n",
    "        # The lambda function takes an arg (x) and we can give it every row and in the sense we end up processing the whole df\n",
    "        # \n",
    "        # )\n",
    "\n",
    "        #########################################################################################################\n",
    "\n",
    "        df['Company'] = df.apply(lambda row: row.Email.split('@')[1][: row.Email.split('@')[1].index('.')], axis=1) \n",
    "        \n",
    "        #Here we are adding a new column 'company' that will let us track bot companies\n",
    "        #########################################################################################################\n",
    "\n",
    "\n",
    "        df['name'] = df.apply(lambda row: row.Email.split('@')[0].lower(), axis=1) \n",
    "        fake_people = ['info', 'sales', 'marketing', 'operations']\n",
    "\n",
    "        df = df[~df.name.isin(fake_people)].reset_index(drop=True)\n",
    "\n",
    "        people_clicks = {}\n",
    "\n",
    "        for person in df.Email:\n",
    "            clicks = df[df.Email == person].LinkClickedDate.reset_index(drop =True)\n",
    "        \n",
    "            people_clicks[person] = clicks\n",
    "\n",
    "        #########################################################################################################\n",
    "\n",
    "        \n",
    "        #get clickers that clicked too quickly\n",
    "        bots = []\n",
    "        bot_companies = ['bpl-business']\n",
    "        for clicker in people_clicks:\n",
    "            clicks = list(people_clicks[clicker])\n",
    "            clicks.sort()\n",
    "            \n",
    "\n",
    "            \n",
    "            too_soon = timedelta(seconds=1)\n",
    "            shortest_time = timedelta(seconds=-1)\n",
    "\n",
    "            length = len(clicks)\n",
    "            if length:\n",
    "                for i in range(length-1):\n",
    "                    gap = clicks[i+1]- clicks[i]\n",
    "                \n",
    "                    if (gap > shortest_time and gap <= too_soon):\n",
    "                        if not clicker in bots:\n",
    "                            bots.append(clicker)\n",
    "\n",
    "                        comp = df[df.Email == clicker].Company.reset_index(drop=True)[0] \n",
    "                        if not comp in bot_companies:\n",
    "                            bot_companies.append(comp)\n",
    "\n",
    "                        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        sanitised = df[~df.Email.isin(bots) & ~df.Company.isin(bot_companies)].reset_index(drop=True)\n",
    "\n",
    "        ##make holding directories\n",
    "\n",
    "        parent_dir = f\"{os.getcwd()}\\\\Completed_Reports\"\n",
    "        \n",
    "\n",
    "        dir_path = f\"{os.getcwd()}\\\\Completed_Reports\\\\{company}\"\n",
    "\n",
    "        dir_exists = os.path.isdir(dir_path)\n",
    "\n",
    "        if not dir_exists:\n",
    "            os.mkdir(dir_path)\n",
    "        else:\n",
    "            os.mkdir(f\"{os.getcwd()}\\\\Completed_Reports\\\\{company}\\\\{time_stamp}\")\n",
    "            dir_path = f\"{os.getcwd()}\\\\Completed_Reports\\\\{company}\\\\{time_stamp}\"\n",
    "            \n",
    "        sanitised.to_csv(f\"{dir_path}\\\\{subject_line}-clickStats.csv\",index=False)\n",
    "        generic_df.to_csv(f'{dir_path}\\\\{subject_line}-genericStats.csv', index=False)\n",
    "\n",
    "        links = {}\n",
    "        for link in sanitised.LinkUrl:\n",
    "            if link in links:\n",
    "                links[link] +=1\n",
    "            else:\n",
    "                links[link] =1\n",
    "\n",
    "\n",
    "        links_Y = []\n",
    "        links_Xticks = []\n",
    "        color_scheme =[]\n",
    "        colors = colours\n",
    "        index = 0\n",
    "\n",
    "\n",
    "\n",
    "        for x in links:\n",
    "            label = x\n",
    "            links_Xticks.append(f\"Link: {label}, {links[x]} click(s)\")\n",
    "            links_Y.append(links[x])\n",
    "            if index % 2 ==0:\n",
    "                color_scheme.append(colors[0])\n",
    "            else:\n",
    "                color_scheme.append(colors[1])\n",
    "            index +=1\n",
    "\n",
    "        if (len(links)<2):\n",
    "            links_Xticks.append(f\"No other links clicked\")\n",
    "            links_Y.append(0)\n",
    "            color_scheme.append(colors[1])\n",
    "\n",
    "\n",
    "        print(color_scheme)\n",
    "        plt.barh(links_Xticks, links_Y, color=color_scheme)\n",
    "        plt.title(f'Click stats for {company} campaign: {subject_line}')\n",
    "        plt.xlabel('clicks')\n",
    "        plt.savefig(f\"{dir_path}\\\\{subject_line}-ecast-stats.pdf\", bbox_inches = 'tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_basic_info(company, stats_url):\n",
    "\n",
    "    ## Login to dynamail\n",
    "    webpage_response = req.get(credentials[company]['url'])\n",
    "    webpage = webpage_response.content\n",
    "    # login_page = BeautifulSoup(webpage,'html.parser')\n",
    "\n",
    "        ##login input fields\n",
    "\n",
    "    login_creds = {\n",
    "        'ctl00$CPHP$newloginControl$tbUsername': username,\n",
    "        'ctl00$CPHP$newloginControl$tbPassword': credentials[company]['password'],\n",
    "        'ctl00$CPHP$newloginControl$loginButton': 'Login',\n",
    "        '__VIEWSTATEGENERATOR': '5E82A05A',\n",
    "        '__VIEWSTATE': credentials[company]['__VIEWSTATE'],\n",
    "        '__EVENTVALIDATION': credentials[company]['__EVENTVALIDATION'],\n",
    "    }\n",
    "\n",
    "    session = req.session()\n",
    "    res = session.post(credentials[company]['url'], data=login_creds)\n",
    "\n",
    "\n",
    "    ####################################################################\n",
    "    #   Go to specific url#######\n",
    "    stats_page = session.get(stats_url)\n",
    "    html = './index.html'\n",
    "    stats_page_soup = BeautifulSoup(stats_page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    generic_stats ={\n",
    "        'Sent': [0],\n",
    "        'Opens':[0],\n",
    "        'Individual Opens':[0]\n",
    "    }\n",
    "\n",
    "    client_info = {\n",
    "        'subject_line': '',\n",
    "        'company':''\n",
    "    }\n",
    "\n",
    "\n",
    "    ####get total_Sent\n",
    "    labels = stats_page_soup.find_all(attrs={'class':'Label'})\n",
    "\n",
    "    for x in labels:\n",
    "\n",
    "        if(x.string.find('Emails') > 0):   \n",
    "            generic_stats['Sent'][0] = int(x.parent.find_all('td')[1].find('a').string)\n",
    "        if(x.string.find('Subject') > 0):\n",
    "            client_info['subject_line'] = x.parent.find_all('td')[1].string\n",
    "        if(x.string.find('Campaign') > 0):\n",
    "            client_info['company'] = x.parent.find_all('td')[1].find('a').string\n",
    "        \n",
    "\n",
    "    ##### get opened and unique opens\n",
    "    unique_opens_img = stats_page_soup.find('img', attrs={'id': 'ctl00_CPHP_mailshotCharts_EmailsByStatusChart_Chart1'}).parent.find_all('area')[1].attrs['title']\n",
    "    unique_opens = int(unique_opens_img[unique_opens_img.index('- ')+1:])\n",
    "    generic_stats['Individual Opens'][0] = unique_opens\n",
    "\n",
    "    overall_opens = int(stats_page_soup.find('span', attrs={'id':'ctl00_CPHP_mailshotCharts_OpenedEmailsLabel'}).find('b').text)\n",
    "    generic_stats['Opens'][0] = overall_opens\n",
    "\n",
    "    ### santise subject line and subject_line\n",
    "    start_slice = inf\n",
    "    end_slice = None\n",
    "    for x in client_info['subject_line']:\n",
    "        if(x != ' ' and x !='\\n' and x != '\\r\\n' and x != '\\r'):\n",
    "            end_slice = client_info['subject_line'].rindex(x)\n",
    "            if(client_info['subject_line'].index(x) < start_slice):\n",
    "                start_slice = client_info['subject_line'].index(x)\n",
    "    client_info['subject_line'] = remove_characters(client_info['subject_line'][start_slice:end_slice+1])\n",
    "\n",
    "\n",
    "    ### santise company and subject_line\n",
    "    start_slice = inf\n",
    "    end_slice = None\n",
    "    for x in client_info['company']:\n",
    "        if(x != ' ' and x !='\\n' and x != '\\r\\n' and x != '\\r'):\n",
    "            end_slice = client_info['company'].rindex(x)\n",
    "            if(client_info['company'].index(x) < start_slice):\n",
    "                start_slice = client_info['company'].index(x)\n",
    "    client_info['company'] = remove_characters(client_info['company'][start_slice:end_slice+1])\n",
    "\n",
    "    generic_df = pd.DataFrame(generic_stats)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(generic_stats, client_info)\n",
    "\n",
    "    clicks_table = stats_page_soup.find('table', attrs={'class':'PlainTable'}).find_all('tr')\n",
    "\n",
    "    individual_clicks = {\n",
    "        'pixels':[],\n",
    "        'company':[]\n",
    "    }\n",
    "    for index in range(1, len(clicks_table)):\n",
    "        row = clicks_table[index]\n",
    "        sections = row.find_all('td')\n",
    "        \n",
    "        link_clicked = sections[1]\n",
    "        link_clicked_href = link_clicked.find('a').get('href')\n",
    "\n",
    "        click_stats_link = sections[2].find('a').get('href')\n",
    "\n",
    "\n",
    "        # print(sections[1].find('a').get('href'))\n",
    "        if link_clicked_href in credentials[company]['bot_links']:\n",
    "            if not click_stats_link in individual_clicks['pixels']:\n",
    "                individual_clicks['pixels'].append(click_stats_link)\n",
    "        else:\n",
    "            if not click_stats_link in individual_clicks['company']:\n",
    "                individual_clicks['company'].append(click_stats_link)\n",
    "\n",
    "    os.mkdir(f'{os.getcwd()}\\\\company')\n",
    "    os.mkdir(f'{os.getcwd()}\\\\pixels')\n",
    "    for index in range(len(individual_clicks['pixels'])):\n",
    "        link = individual_clicks['pixels'][index]\n",
    "        click_stats_res = session.get(link)\n",
    "        click_stats_page = BeautifulSoup(click_stats_res.content, 'html.parser')\n",
    "        form = click_stats_page.find('form')\n",
    "        inputs = form.find_all('input')\n",
    "        input_dictionary={}\n",
    "        for x in inputs:\n",
    "           name= x.get('name')\n",
    "           value = x.get('value')\n",
    "           if(name != None and not name in input_dictionary):\n",
    "            input_dictionary[name]=value\n",
    "        csv_data = session.post(link, data=input_dictionary)\n",
    "        newFile = open(f'{os.getcwd()}\\\\pixels\\\\Recipients({index}).csv', 'w', newline='')\n",
    "        newFile.write(csv_data.text)\n",
    "        newFile.close()\n",
    "\n",
    "    for index in range(len(individual_clicks['company'])):\n",
    "        link = individual_clicks['company'][index]\n",
    "        click_stats_res = session.get(link)\n",
    "        click_stats_page = BeautifulSoup(click_stats_res.content, 'html.parser')\n",
    "        form = click_stats_page.find('form')\n",
    "        inputs = form.find_all('input')\n",
    "        input_dictionary={}\n",
    "        for x in inputs:\n",
    "           name= x.get('name')\n",
    "           value = x.get('value')\n",
    "           if(name != None and not name in input_dictionary):\n",
    "            input_dictionary[name]=value\n",
    "        csv_data = session.post(link, data=input_dictionary)\n",
    "        newFile = open(f'{os.getcwd()}\\\\company\\\\Recipients({index}).csv', 'w', newline='')\n",
    "        newFile.write(csv_data.text)\n",
    "        newFile.close()\n",
    "\n",
    "\n",
    "    ##run analysis\n",
    "    if client_info['company'] in clients:\n",
    "        analysis_and_visualisation(subject_line=client_info['subject_line'], company=client_info['company'], colours=clients[client_info['company']], generic_df=generic_df)\n",
    "    else:\n",
    "        print(f'To run analysis please provide colours for the company: {client_info[\"company\"]}')\n",
    "get_basic_info(company, stats_url=stats_url)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
